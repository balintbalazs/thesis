% !TEX root = dissertation_BB.tex
%% spellcheck-language en-US

%   #
%  ##
%   #
%   #
%  ###

\chapter{Live imaging in three dimensions}
\label{ch:intro}

\graphicspath{{./figures/1_spim/}}


Live imaging is indispensable to understand the processes at the interface of cell and developmental biology. In an ideal setting, the ultimate microscope would be able to record a continuos, three dimensional (3D), multicolor dataset of any biological process of interest with the highest possible resolution. Due to several limitations in physics and biology this is not possible. Therefore, a compromise is necessary. The diffractive nature of light, the lifetime of fluorescent probes and the photosensitivity of biological specimens all require microscopy to be adapted so that the question at hand may be answered.

In order to acquire useful data, one has to choose a tradeoff between spatial and temporal resolution and signal contrast, while making sure the biology is not affected by the imaging process itself \cite{laissue_assessing_2017}. This challenge can be illustrated by a pyramid where each corner represents one of these criteria (\autoref{fig:tradeoffs}), while a point inside the pyramid corresponds to the imaging conditions. As soon as we try to optimize one condition, \textit{i.e.}, we move the point closer to one of the corners, it will move further from all the others due to the limited photon budget. In order to make a fundamental difference and move the corners of the pyramid closer together, an change in microscope design is necessary.

\begin{figure}[bht]
  \centering
  \includegraphics[width=0.4\textwidth]{tradeoffs}
  \bcaption[Tradeoffs in fluorescence microscopy for live imaging]{Also called the ``pyramid of frustration". When optimizing the imaging conditions (red dot), a tradeoff has to be made between resolution, contrast, and imaging speed, while avoiding photodamage. One can only be improved at the expense of the others due to the limited photon budget of the fluorescent molecules. Adapted from \cite{laissue_assessing_2017}.}
  \label{fig:tradeoffs}
\end{figure}


% Light microscopy is one of the oldest methods that is still widely used today to investigate the inner workings of microscopic life. A particularly important branch is fluorescence microscopy \cite{diaspro_optical_2011}, that relies on using fluorescent labels that mark specific structures inside the specimens. 

\section{Wide-field fluorescence microscopy}

Fluorescence microscopy \cite{lichtman_fluorescence_2005,diaspro_optical_2011}, as a subset of light microscopy, is one of the few methods that allow subcellular imaging of live specimens with specific labeling.
%of structures of interest.
The first use of the term fluorescence is credited to  George Gabriel Stokes \cite{stokes_change_1852}, and it refers to the phenomenon of light emission following the absorption of light or other electromagnetic radiation. As the name fluorescence microscopy suggests, this method collects fluorescent light from the specimens.
% which has numerous advantages, but also some drawbacks.
Since biological tissues are usually not fluorescent, except for some autofluorescence mostly at shorter wavelengths, fluorescent dyes or proteins have to be introduced to the system in order to be able to collect the necessary information. The advantage of this is that the signal of the labeled structures will be of very high ratio compared to the background.

A fluorescent molecule is capable of absorbing photons in a given wavelength range (excitation spectrum) and temporarily store its energy by having an electron in a higher energy state, \textit{i.e.}, in an excited state. This excited state, however, is not stable, and the electron quickly returns to the ground state while emitting a photon. The energy of the absorbed and emitted photons are not the same, as energy loss occurs due to internal relaxation events, and the emitted photon has lower energy than the absorbed photon. This phenomenon is called the Stokes shift, or red shift, and can be exploited in microscopy to drastically increase the signal-to-noise ratio by filtering out the illumination light (\autoref{fig:spectrum}).

  \begin{figure}
    \centering
    \includegraphics[width=0.6\textwidth]{spectrum/egfp}
    \bcaption[Excitation and emission spectrum of enhanced green fluorescent protein (EGFP)]{Excitation spectrum in blue, emission spectrum in yellow. The separation between the two spectra is due to the Stokes shift, which is 19 nm for EGFP. Emission and excitation light can be separated by a long-pass filter at \SI{500}{nm}. Data from \cite{noauthor_spectra_nodate}.}
    \label{fig:spectrum}
  \end{figure}

  % Although this process can disturb the natural environment 

  %  very small amount of illumination photons will result in fluorescence ($<0.0001\%$), the signal-to-noise ratio of the fluorescence is still very high due to the filtering.

  \subsection{Fluorescent proteins}
    Traditionally, synthetic fluorescent dyes were used to label certain structures in the specimens. Some of these directly bind to their target,
    % such as DAPI to DNA,
    and others can be used when conjugated to an antibody specific to the structure or protein of interest. A requirement for these methods is that the fluorescent label has to be added to the sample from an external source, and, in many cases, this also necessitates sample preparation techniques incompatible with live imaging, such as fixation \cite{bacallao_guiding_1990}.

    The discovery of fluorescent proteins has revolutionized fluorescence microscopy. Since these molecules are proteins, they can be produced directly by the organism if the proper genetic modifications are performed. Even though this was a hurdle at the time of discovering the green fluorescent protein (GFP) \cite{shimomura_extraction_1962}, genetic engineering techniques evolved since then \cite{prasher_primary_1992}, and not only has its gene been successfully integrated in the genome of a multitude of organisms \cite{chalfie_green_1994,amsterdam_aequorea_1995,okabe_green_1997}, but many variants have been also engineered by introducing mutations to increase fluorescence intensity, and to change the fluorescence spectrum to allow multicolor imaging \cite{heim_wavelength_1994,heim_engineering_1996,cormack_facs-optimized_1996,okabe_green_1997}. The usefulness and impact of these proteins are so profound, that in 2008 the Nobel Prize in chemistry was awarded to Osamu Shimomura, Martin Chalfie, and Roger Tsien ``for the discovery and development of the green fluorescent protein, GFP" \cite{service_three_2008}.


  \subsection{Wide-field image formation}
    By imaging fluorescently labelled specimens, a wide-field fluorescence microscope has the capability of discriminating illumination light from emitted fluorescent light due to the Stokes shift described in the previous section. The microscope's operating principle is depicted in \autoref{fig:wide-field}.

    Light from a source, typically a mercury lamp is focused on the back focal plane of the objective to create even illumination of the sample. Before entering the objective, the light is filtered, so only the wavelengths that correspond to the excitation properties of the observed fluorophores are transmitted. Since the same objective is used for both illumination and detection, a dichroic mirror is utilized to decouple the illumination and detection paths. The emitted light is filtered again to make sure any reflected and scattered light from the illumination source is blocked to increase signal-to-noise ratio.
    Finally, light is focused by a tube lens to create a magnified image on the camera sensor.
    
    This type of arrangement is called infinity-corrected optics, since the back focal point of the objective is in ``infinity", meaning that the light from a point source exiting the back aperture is parallel. This is achieved by placing the sample exactly at the focal point of the objective. Infinity-corrected optics have the advantage that they allow placing various additional optical elements in the infinity space, (\textit{i.e.}, the space between the objective and the tube lens) without affecting the image quality. In this example such elements are the dichroic mirror and the emission filter. 

    \begin{figure}[tb]
      \centering
      \includegraphics[page=4,width=0.7\textwidth]{spim_cyl}
      \bcaption[Wide-field fluorescence microscope]{The light source is focused on the back focal plane of the objective to provide an even illumination to the sample. Emitted photons are collected by the objective, and are separated from the illumination light by a dichroic mirror. Inset: Light collection of an objective lens. $\alpha$: light collection half-angle; $f$: focal length; $r$: radius of aperture.}
      \label{fig:wide-field}
    \end{figure}


    The combination of the objective and tube lens together will determine the optical magnification of the system, which will be the ratio of the focal lengths of these lenses:
    \begin{equation}
      M = \frac{f_{TL}}{f_{OBJ}}.
      \label{eq:magnification}
    \end{equation}
    The final field of view (FOV) of the microscope will depend on the magnification, the size of the imaging sensor ($D$), and the objective field number ($FN$, specified by the manufacturer, the diameter of the view field in the image plane):
    \begin{equation}
      FOV = \frac{\min(D, FN)}{M}.
      \label{eq:FOV}
    \end{equation}

    Apart from the magnification, the most important property of the objective is the half-angle of the light acceptance cone, $\alpha$ (\autoref{fig:wide-field}, inset). This not only determines the amount of collected light, but also the achievable resolution of the system (see \autoref{sec:resolution}). This angle depends on the size of the lens relative to its focal length. In other words it depends on the aperture of the lens, which is why the expression \textit{numerical aperture} (NA) is more commonly used to express this property of the objective:
    \begin{equation}
      \NA = n\cdot \sin \alpha,
      \label{eq:NA}
    \end{equation}
    where $n$ is the refractive index of the medium, and describes the light propagation speed in the medium relative to the speed of light in vacuum. For vacuum and air $n=1$, for water $n_\mathrm{H_2O}=1.33$, and for the commonly used optical glass BK7 $n_\mathrm{BK7} = 1.52$.

    For small $\alpha$ angles, the following approximation holds true: $\sin \alpha \approx \tan \alpha \approx \alpha$. Thus, the numerical aperture can also be expressed as a ratio of the radius of the lens and the focal length:
    \begin{equation}
      \NA \approx n \frac{r}{f},\quad \text{when }\alpha \ll 1.
    \end{equation}
    % This expression also shows the relationship of the numerical aperture and the f-number commonly used in photography to characterize a lens's aperture:
    % \begin{equation}
    %     f\# = \frac{f}{d} \approx \frac{2}{n\cdot NA}
    % \end{equation}


    % ########  ########  ######  
    % ##     ## ##       ##    ## 
    % ##     ## ##       ##       
    % ########  ######    ######  
    % ##   ##   ##             ## 
    % ##    ##  ##       ##    ## 
    % ##     ## ########  ######  

  \subsection{Resolution of a wide-field microscope}
    \label{sec:resolution}
    The resolution of an optical system depends on the size of the smallest distinguishable feature on the image. A practical way of quantifying this is by measuring the smallest resolved distance, \textit{i.e.}, the minimum distance between two point-like objects so that the two objects can still be distinguished. This mainly depends on two factors: the NA of the objective, and the pixel size of the imaging sensor.

    Even if the imaging sensor would have infinitely fine resolution, it is not possible to reach arbitrarily high resolutions due to the wave nature of light and diffraction effects that occur at the aperture of the objective. This means that depending on the wavelength of the light, any point source will have a finite size on the image, it will be spread out, limiting the resolution. The shape of this image is called the \textit{point spread function}, or PSF (\autoref{fig:psf-wf}), as this function describes the behavior of the optical system when imaging a point-like source. This property of lenses was already discovered by Abbe in 1873 \cite{abbe_beitrage_1873}, when he constructed his famous formula:
    \begin{equation}
      \delta = \frac{\lambda}{2 \cdot \NA}.
      \label{eq:abbe}
    \end{equation}
    where $\delta$ is the smallest distance between two distinguishable features.

    Another representation of the optical performance, is the \textit{optical transfer function}, or OTF (\autoref{fig:psf-wf}), which is the Fourier transform of the PSF:
    \begin{equation}
      \text{OTF} = \mathcal{F}(\text{PSF}).
    \end{equation}
    As this function operates in the frequency space, it describes how the different spatial frequencies are affected by the system. The resolution can also be defined as the maximum of the support of the OTF, since this describes the highest frequency that is still transmitted by the optical system. Any pattern with higher frequency will be lost, thus lies beyond the resolution limit. For circularly symmetric PSFs, the OTF will have real values. However, if this is not the case, the Fourier transform also introduces complex components.

    \begin{figure}
      \centering
      \includegraphics[width=1\textwidth]{psfs/WF.pdf}
      \bcaption[Axial cross section of the PSF and OTF of a wide-field microscope]{Simulated PSF (left) and OTF (right) for a wide-field microscope with a water immersion objective ($n=1.33$). $NA=1.1$, $\lambda = \SI{510}{nm}$. Intensity has been normalized relative to the maximum, and is visualized with different colors (see colorbar). For better visualization, the logarithm of the intensity is displayed for the PSF.}
      \label{fig:psf-wf}
    \end{figure}


    Abbe's formula can be derived from the scalar theory of diffraction using a paraxial approximation (Fraunhofer diffraction, \cite{born_principles_2013}).
    It is useful to define the following optical coordinates instead of the commonly used Cartesian coordinates $x$, $y$ and $z$:
    \begin{equation}
      v = \frac{2\pi n  r}{\lambda_0} \sin \alpha, \quad
      u=\frac{8\pi n  z}{\lambda_0} \sin^2 \frac{\alpha}{2}
      \label{eq:substitutions}
    \end{equation}
    where $r = \sqrt{x^2 + y^2}$ is the distance from the optical axis, and $\alpha$ is the light collection angle as shown on \autoref{fig:wide-field}. In this system the intensity of the electric field in the focus of a lens is \cite{sheppard_imaging_1987}:
    \begin{equation}
      H(u,v) = C_0 \left| \int_0^1 J_0 (v\rho)e^{-i\frac{1}{2}\cdot u\rho^2} \rho d \rho \right|^2,
      \label{eq:psf-born-wolf}
    \end{equation}
    where $C_0$ is a normalization constant,  $\rho = r / \max(r)$ is the normalized distance from the optical axis, and $J_0$ is the zeroth-order Bessel function of the first kind. This is also called the Born-Wolf PSF model, named after the original authors \cite{born_principles_2013}.
    
    

    To determine the lateral resolution of the system, let's substitute $u=0$ as the axial optical coordinate, and evaluate \autoref{eq:psf-born-wolf} which will give the intensity distribution in the focal plane:
    \begin{equation}
      H(0,v) = C_0 \left| \int_0^1 J_0(v\rho)\rho d \rho \right|^2 = \left(2\frac{J_1(v)}{v} \right) ^2,
      \label{eq:airy}
    \end{equation}
    \begin{figure}
      \centering
      \includegraphics[width=0.7\textwidth]{airy}
      \bcaption[Airy pattern]{Airy pattern calculated in Matlab based on \autoref{eq:airy}.}
      \label{fig:airy}
    \end{figure}


    

    where $J_1$ is the first-order Bessel function of the first kind.
    % potentially first-order Bessel function
    This equation describes the famous Airy pattern (\autoref{fig:airy}) which will be the shape of the PSF in the focal plane. The width of this pattern will define the smallest resolvable distance, and although there are multiple definitions for this, the most commonly accepted is the Rayleigh criterion \cite{f.r.s_xxxi._1879, born_principles_2013}. It defines the resolution as the distance between the central peak and the first local minimum. As this lies at $v=3.38$, the resolution can be expressed by substituting this value into \autoref{eq:substitutions} and solving it for $r$:
    \begin{equation}
      \delta_{xy} = r(v=0.338) = \frac{3.83}{2\pi} \frac{\lambda_0}{n\cdot \sin \alpha} \approx 0.61 \frac{\lambda_0}{NA},
      \label{eq:lateralRes}
    \end{equation}
    which is equivalent to Abbe's original formula (\autoref{eq:abbe}). The only difference is the scaling factor which is due to the slightly different interpretations of the width of the Airy disk as mentioned earlier.

    Similarly, to calculate the intensity distribution along the axial direction, let's substitute $v=0$ into \autoref{eq:psf-born-wolf}:
    \begin{equation}
      H(u,0)=C_0\left( \frac{\sin \frac{u}{4}}{\frac{u}{4}}\right) ^2 . 
    \end{equation} 
    For this expression the first minimum lies at $u=4\pi$. Converting back to Cartesian coordinates, the axial resolution can be expressed as:
    \begin{equation}
      \delta_z = \frac{2n\lambda_0}{NA^2}.
      \label{eq:axialRes}
    \end{equation}

    So far we only considered a single, point-like emitter. As the intensity function describes how an optical system ``spreads out" the image of a point, it is also called the Point Spread Function (PSF, \autoref{fig:psf-wf}). In a more realistic scenario, however the emitters are neither point-like, nor single. Effectively, however, for every emitter the PSF would be imaged on the sensor, and this creates the final image. In mathematical terms, this can be expressed as a convolution operation between the underlying fluorophore distribution of the object ($O$) and the PSF ($H$):
    \begin{equation}
      I(u,v) = O(u,v) * H(u,v).
    \end{equation}

    The effective result of this kind of diffraction-limited image formation is a blurred image with a finite resolution of $\delta_{xy}$ in the lateral direction, and $\delta_z$ in the axial direction.

    The PSF is further affected by the illumination pattern as well. Since the number of emitted fluorescent photons are roughly proportional to the illumination intensity, the illumination pattern will have an effect on the overall PSF of the system, which can be expressed as:
    \begin{equation}
      H_{sys} = H_{ill} \cdot H_{det},
      \label{eq:systemPSF}
    \end{equation}
    where $H_{ill}$ is the point spread function of the illumination, and $H_{det}$ is the point spread function of the detection.


  \subsection{Simulating the point spread function}
  \label{sec:simu}
    To estimate the performance of a microscope, it is useful to simulate its point spread function. Usually this entails the numerical evaluation of the diffraction integral near the focal point. The most commonly used model for the PSF is the Born-Wolf model, which we already introduced in \autoref{eq:psf-born-wolf}. The only parameters for this model are the wavelength of the light ($\lambda$), the numerical aperture ($\NA$), and the refractive index of the immersion medium ($n$). This model, however only accounts for an ideal image forming system without any aberrations.
    
    For some experiments it is necessary to use the objectives in a slightly different environment than the original design conditions. Changes in the medium refractive index or coverslip thickness can lead to spherical aberrations, which are not accounted for in the Born-Wolf PSF model. The Gibson-Lanni model \cite{gibson_experimental_1992} gives a more general approach and it accounts for any differences between the experimental conditions and the design parameters of the objective, thus, it can simulate any possible aberrations that may arise. The additional adjustable parameters are the thicknesses ($t$) and refractive indices ($n$) of the immersion medium ($i$), coverslip ($g$), and sample ($s$) (\autoref{fig:gibson-lanni}).

    \begin{figure}
      \centering
      \includegraphics[width=0.75\textwidth]{gibson-lanni}
      \bcaption[Optical path length differences in the Gibson-Lanni PSF model]{The optical path length difference is given by $OPD=[ABCD]-[PQRS]$, where $[ABCD]$ is the optical length of the experimental path (red), and $[PQRS]$ is the optical length of the design path (blue). Adapted from \cite{li_fast_2017}.}
      \label{fig:gibson-lanni}
    \end{figure}
    
    Based of the parameters of the system, $\mathbf{p}=(\na, \mathrm{n}, \mathrm{t})$, where $\mathrm{n} = (n_i, n_i^*, n_g, n_g^*, n_s)$ represents the refractive indices, and $\mathrm{t} = (t_i, t_i^*, t_g, t_g^*, t_s)$ represents the medium thicknesses, the optical path difference can be calculated as:
    \begin{equation}
      \begin{split}
        OPD(\rho, z; z_p, \mathbf{p}) = (z+t_i^*) \sqrt{n_i^2-(\na \rho)^2} + z_p \sqrt{n_s^2 - (\na \rho)^2} - \\
        - t_i^* \sqrt{(n_i^*)^2 - (\na \rho)^2} + t_g \sqrt{n_g^2 - (\na \rho)^2} - t_g^* \sqrt{(n_g^*)^2 - (\na \rho)^2},
      \end{split}      
      \label{eq:OPD}
    \end{equation}
    where $\rho = r / \max(r)$ is the normalized distance from the optical axis. Then, the intensity near the focus can be expressed as:
    \begin{equation}
      H(u,v) = C \left| \int_0^1 J_0 (v \rho )e^{iW(\rho, z; z_p, \mathbf{p})} \rho d \rho \right|^2,
      \label{eq:psf-gibson-lanni}
    \end{equation}
    where the phase term $W(\rho, z; z_p, \mathbf{p}) = \frac{2\pi}{\lambda} ODP(\rho, z; z_p, \mathbf{p})$ and $C$ is a constant complex amplitude.

    Multiple software packages offer numerical evaluations of \autoref{eq:psf-born-wolf} and \autoref{eq:psf-gibson-lanni}. The ones extensively used in this thesis are the PSF Generator plugin \cite{kirshner_3d_2011} for Fiji \cite{schindelin_fiji:_2012}, and MicroscPSF \cite{li_fast_2017}, which is implemented in Matlab. The PSF Generator supports the both PSF models, while MicroscPSF supports the Gibson-Lanni model. The latter implementation has the advantage that it is much faster due to the Bessel series approximation of the integral term, however, it only calculates the axial cross-section of the PSF. In contrast, PSF generator evaluates the integral for any specified 3D volume around the focal point.
    




\section{Point scanning methods}
  In most cases, a wide-field microscope is used to image a layer of cultured cells, or a sectioned sample, thus axial resolution is not a concern. Imaging live specimens, however is not so straightforward, as these samples are usually much thicker than a typical section. For these samples 3-dimensional (3D) imaging is highly beneficial, which necessitates the use of optical sectioning instead of physical sectioning to be able to discriminate the features at different depths.


  Due to the design of the wide-field microscope, any photons emitted from outside the focal plane will also be detected by the sensor, however as these are not originating from the focus, only a blur will be visible. This blur potentially degrades image quality and signal-to-noise ratio to such an extent that makes imaging thick samples very difficult if not impossible in a wide-field microscope. 

  % In the previous section we defined optical resolution, and derived the formulas to calculate the lateral and axial resolutions. These formulas, however are not completely accurate for high NA imaging, since the derivation itself depended on a paraxial approximation of the Kirchhoff diffraction equation. A more robust, and generally accepted method to calculate the resolution for high-NA objectives is the Stelzer-Grill-Heisenberg (SGH) theory
  % \cite{grill_method_1999, stelzer_uncertainty_2000}:
  % \begin{equation} \label{eq:latres}
  % \sigma_{xy}=\frac{\lambda}{\sqrt{3-2 \cos \alpha - \cos 2 \alpha}}
  % \end{equation}
  % \begin{equation} \label{eq:axres}
  % \sigma_z = \frac{\lambda}{1-\cos \alpha}
  % \end{equation}

  \begin{figure}
    \centering
    \includegraphics[width=0.55\textwidth]{resolution}
    \bcaption[Resolution of a wide-field microscope]{Axial (blue) and lateral (red) resolutions of a wide-field microscope are shown with respect to the numerical aperture (NA). Resolutions are calculated with $\lambda =510nm$, the emission maximum of GFP and $n=1.33$, the refractive index of water, for water dipping objectives.}
    \label{fig:resolution}
  \end{figure}

  Evaluating Equations \ref{eq:lateralRes} and \ref{eq:axialRes} for a range of possible numerical apertures reveals the significant differences in lateral and axial resolution for any objective (\autoref{fig:resolution}). Especially for low NAs, this can be significant, a factor of $\sim$20 difference. For higher (>0.8) NAs the axial resolution increases faster than the lateral, however they will only be equal when $\alpha=\SI{180}{\degree}$. This means that isotropic resolution with a single lens is only possible if the lens is collecting all light emitting from the sample, which seems hardly possible, and would be highly impractical. For commonly used high NA objectives the lateral to axial ratio will be around 3--6. 

  Instead of using a single lens to achieve isotropic resolution, it is more practical to image the sample from multiple directions to complement the missing information from different views. When rotating the sample by \SI{90}{\degree} for example, the lateral direction of the second view will correspond to the axial direction of the first view. If rotation is not possible, using multiple objectives can also achieve similar results, such as in the case of Multi-Imaging Axis Microscopy (MIAM) \cite{swoger_multiple_2003,swoger_multi-view_2007}. This microscope consisted of 4 identical objectives arranged in a tetrahedral fashion to collect as much light as possible from multiple directions, and provide isotropic 3D resolution, albeit at the expense of extremely difficult sample handling, since the sample was surrounded by objectives from all directions. 

  % Another disadvantage of the wide-field microscope, is that it can not be used with thick specimens. Usually this type of microscopy is only used for a single layer of cells, because all the objects in the field of view will appear on the imaging plane, not just the plane in focus. These objects will appear blurred if close to the focus, or just evenly add to the background noise if they are further from the focus. This is why imaging specimens much thicker than $10\ \mu m$ will result in suboptimal image quality.





%  ####    ####   #    #  ######   ####    ####     ##    #      
% #    #  #    #  ##   #  #       #    #  #    #   #  #   #      
% #       #    #  # #  #  #####   #    #  #       #    #  #      
% #       #    #  #  # #  #       #    #  #       ######  #      
% #    #  #    #  #   ##  #       #    #  #    #  #    #  #      
%  ####    ####   #    #  #        ####    ####   #    #  ###### 
                                                              

  \subsection{Confocal laser scanning microscopy}

    Confocal laser scanning microscopy (CLSM) \cite{minsky_microscopy_1961,davidovits_scanning_1969} addresses most of the problems of wide-field microscopy we mentioned in the previous section. It is capable of optical sectioning by rejecting out-of-focus light, which makes it a
    true % what?
    3D imaging technique. Furthermore, the light rejection also massively reduces out-of-focus background, and increases contrast.

    This is achieved by two significant modifications compared to the wide-field optical path. To be able to reject the out-of-focus light, an adjustable pinhole is placed at the focus of the tube lens. Light rays originating from the focal point will meet here, and are able to pass through the pinhole. However, out-of-focus light will converge either before or after the aperture, and thus the aperture blocks these rays. To maximize the fluorescence readout efficiency for the single focal point, a photomultiplier tube is used instead of an area sensor (\autoref{fig:confocal}).

    \begin{figure}[tb]
    \begin{subfigure}[t]{0.49\textwidth}
      \centering
      \includegraphics[page=3,width=\textwidth]{spim_cyl}
      \caption{\textbf{Confocal microscope}}
      \label{fig:confocal}
    \end{subfigure}
    \begin{subfigure}[t]{0.49\textwidth}
      \centering
      \includegraphics[page=5,width=\textwidth]{spim_cyl}
      \caption{\textbf{Confocal-theta microscope}}
      \label{fig:conf-theta}
    \end{subfigure}
    \bcaption[Basic optical components of a confocal laser scanning and confocal-theta microscope]{Both types of microscopes use confocal image detection, which means that a pinhole is used to exclude light coming from out-of-focus points. Light intensity is measured by a photomultiplier for every point in the region of interest. The final image is generated on a computer using the positions and recorded intensity values. A regular confocal microscope (a) uses the same objective for illumination and detection, while a confocal-theta microscope (b) uses a second objective that is rotated by $\theta$ around the focus. In this case, $\theta = 90^\circ$.}
    \label{fig:confocals}
    \end{figure}

    As only a small focal volume is detected at a time, the illumination light is also focused here by coupling an expanded laser beam through the back aperture of the objective. This not only increases illumination efficiency (since other, not detected points are not illuminated), but has the added benefit of increasing the resolution as well. This is due to the combined effect of illumination and detection PSFs as described in \autoref{eq:systemPSF} (\autoref{fig:psf-confocal}). For Gaussian-like PSFs, the final resolution (along a single direction) can be calculated in the following way:
    \begin{equation}
      \frac{1}{\delta _{sys}^2} = \frac{1}{\delta _{ill}^2} + \frac{1}{\delta _{det}^2},
      \label{eq:systemRes}
    \end{equation}
    where $\delta_{ill}$ and $\delta_{det}$ are the resolutions for the illumination and detection, respectively. Since the same objective is used for both illumination and detection, and the difference in wavelength is almost negligible, $\delta_{ill} = \delta_{det} = \delta$, the final system resolution will be:
    \begin{equation}
      \delta_{sys} = \frac{1}{\sqrt{2}} \delta.
    \end{equation}
    This means that the distinguishable features in a confocal microscope are $\sim$0.7 times smaller than in a wide-field microscope using the same objective.

    Because of the different detection method in a confocal microscope, direct image formation on an area sensor is not possible, since at any given time only a single point is interrogated in the sample. Instead, it is necessary to move the illumination and detection point in synchrony (or in a simpler, albeit slower solution, to move the sample) to scan the entire field of view. The image can be later computationally reconstructed by a computer program that records the fluorescence intensity of every point in the field of view, and displays these values as a raster image.


    \begin{figure}
      \centering
      \includegraphics[width=1\textwidth]{psfs/confocal.pdf}
      \bcaption[Axial cross section of the PSF and OTF of a confocal laser scanning microscope]{Simulated PSF and OTF for a laser scanning confocal microscope with a water immersion objective ($n=1.33$). $NA=1.1$, $\lambda = \SI{510}{nm}$. Intensity has been normalized relative to the maximum, and is visualized with different colors (see colorbar). For better visualization, the logarithm of the intensity is displayed for the PSF.}
      \label{fig:psf-confocal}
    \end{figure}


  \subsection{Variants of confocal microscopy}

    Although confocal microscopy already has 3D capabilities, its axial resolution is still limited compared to the lateral, since it uses only one objective. An alternative realization of the confocal microscope, the confocal theta microscope \cite{stelzer_fundamental_1994} introduces a second objective to the system that is used to illuminate the sample (\autoref{fig:conf-theta}). Since this decouples the illumination and detection, using a dichroic mirror is no longer necessary. The second objective is rotated by $\theta$ around the focus, this is where the name of this setup originates from.

    As in the case of standard confocal microscopy, the system PSF is improved by the illumination pattern. Here, however, the axial direction of the detection coincides with the lateral direction of the illumination, which results in a dramatic improvement of axial resolution compared to standard confocal microscopy. Lateral resolution will also be increased, but by a smaller extent, resulting in an almost isotropic PSF and equal axial and lateral resolutions. % (\autoref{fig:psf-theta}).
    Although this is a big improvement to confocal microscopy in terms of resolution, this technique did not reach a widespread adoption as it complicates sample handling, while still suffering from two drawbacks of confocal microscopy that limit its live imaging capabilities, namely phototoxicity and imaging speed.

    

    One improvement to address these drawbacks is the use of a spinning disk with a specific pattern of holes (also called Nipkow disk) to generate multiple confocal spots at the same time \cite{graf_live_2005}. If these spots are far enough from each other, confocal rejection of out-of-focus light can still occur. As the disk is spinning, the hole pattern will sweep the entire field of view, eventually covering all points \cite{kino_intermediate_1990}. The image is recorded by an area detector, such as a CCD or EM-CCD, which speeds up image acquisition \cite{nakano_spinning-disk_2002}.

  % \subsection{Mammalian imaging with confocal microscopy}
    


%  ######  ########  #### ##     ## 
% ##    ## ##     ##  ##  ###   ### 
% ##       ##     ##  ##  #### #### 
%  ######  ########   ##  ## ### ## 
%       ## ##         ##  ##     ## 
% ##    ## ##         ##  ##     ## 
%  ######  ##        #### ##     ## 


\section{Light-sheet microscopy}
  \label{sec:light-sheet}
  \begin{figure}[bt]
    \centering
    \includegraphics[width=0.6\textwidth]{spim_concept}
    \bcaption[Basic concept of single-plane illumination microscopy]{The sample is illuminated from the side by laser light shaped to a light-sheet (blue). This illuminates the focal plane of the detection lens, that collects light in a wide-field mode (yellow). The image is recorded, and the sample is translated through the light-sheet to acquire an entire 3D stack.}
    \label{fig:spim_concept}
  \end{figure}

  A selective-plane illumination microscope (SPIM) uses a light-sheet to illuminate only a thin section of the sample (\autoref{fig:spim_concept}). This illumination plane is perpendicular to the imaging axis of the detection objective and coincides with the focal plane. This way, only the section in focus will be illuminated, thus providing much better signal-to-noise ratio. In case of conventional wide-field fluorescence microscopy, where the whole specimen is illuminated, out-of-focus light contributes to a significant background noise. 
  With selective-plane illumination, this problem is intrinsically solved, and it also provides a true optical sectioning capability. This makes SPIM especially suitable for 3D imaging.

  

  The main principle behind single-plane illumination microscopy, that is illuminating the sample from the side by a thin light-sheet, dates back to the early 20\textsuperscript{th} century, when Siedentopf and Zsigmondy first described the ultramicroscope \cite{siedentopf_uber_1902}. This microscope used sunlight as an illumination source that was guided through a precision slit to generate a thin light-sheet. This allowed Zsigmondy to visualize gold nanoparticles floating in and out of the light-sheet by detecting the scattered light from the particles.
  Since these particles were much smaller than the wavelength of the light, the device was called an ultramicroscope. His studies with colloids and the development of the ultramicroscope led Zsigmondy to win the Nobel Prize in 1925.

  After Zsigmondy this method was forgotten until rediscovered in the 1990s, when Voie \etal constructed their Orthogonal-plane Fluorescent Optical Sectioning (OPFOS) microscope \cite{voie_orthogonal-plane_1993}. They used it to image a fixed, optically cleared and fluorescently labelled guinea pig cochlea. In order to acquire a 3D dataset, the sample was illuminated from the side with a light-sheet generated by a cylindrical lens, then rotated around the center axis to obtain multiple views. Although they only reached a lateral resolution of around \SI{10}{\micro m} and axial resolution of \SI{26}{\micro m}, this method allowed them to generate a 3D reconstruction of the guinea pig cochlea \cite{voie_three-dimensional_1995}.

  Later, in 2002, Fuchs et al. developed Thin Light-Sheet Microscopy (TLSM) \cite{fuchs_thin_2002} and used this technique to investigate the microbial life in seawater samples without disturbing their natural environment (by, \textit{e.g.}, placing them on a coverslip). Their light-sheet was similar to the one utilized in OPFOS, being \SI{23}{\micro m} thin, and providing a $\SI{1}{mm} \times \SI{1}{mm}$ field of view.

  Despite these early efforts, the method did not gain larger momentum. The real breakthrough in light-sheet imaging happened at the European Molecular Biology Laboratory (EMBL) in 2004, where Huisken \etal \cite{huisken_optical_2004} combined the advantages of endogenous fluorescent proteins and the optical sectioning capability of light-sheet illumination to image Medaka fish embryos, and the complete embryonic development of a \textit{Drosopila melanogaster} embryo. They called this Selective-Plane Illumination Microscopy (SPIM), and it quickly became popular to investigate developmental biological questions.

  \label{sec:multiview}
  \begin{figure}[tb]
    \centering
    \includegraphics[width=\textwidth]{spim_zoo}
    \bcaption[Different optical arrangements for light-sheet microscopy]{\textbf{(a)} Original SPIM design with a single lens for detection and illumination. \cite{huisken_optical_2004} \textbf{(b)} Upright SPIM to allow for easier sample mounting such as using a petri dish (iSPIM, \cite{capoulade_quantitative_2011, wu_inverted_2011, hoyer_breaking_2016}). \textbf{(c)} Inverted SPIM, where the objectives are below the sample, which is held by a thin foil \cite{strnad_inverted_2016}. \textbf{(d)} Dual-view version of the upright configuration, where both objective can be used for illumination and detection (diSPIM, \cite{wu_spatially_2013}). \textbf{(e)} Multidirectional-SPIM (mSPIM) for even illumination of the sample with two objectives for illumination \cite{huisken_even_2007}. \textbf{(f)} Multi-view SPIM with two illumination and detection objectives for \textit{in toto} imaging of whole embryos (MuVi-SPIM \cite{krzic_multiview_2012}, SimView \cite{tomer_quantitative_2012}, Four-lens SPIM \cite{schmid_high-speed_2013}). \textbf{(g)} A combination of (d) and (f), using four identical objectives, where both can illuminate and detect in a sequential manner, to achieve isotropic resolution without sample rotation (IsoView \cite{chhetri_whole-animal_2015}).}
    \label{fig:spim_zoo}
  \end{figure}
  
  Since then, light-sheet-based imaging has gained more and more popularity, as it can be adapted and applied to a wide variety of problems. Although sample mounting can be challenging because of the objective arrangement, this can also be an advantage, since new microscopes can be designed with the sample in mind \cite{huisken_selective_2009, de_medeiros_light-sheet_2016} (\autoref{fig:spim_zoo}). This made it possible to adapt the technique for numerous other specimens, such as zebrafish larvae \cite{keller_reconstruction_2008}, \textit{C. elegans} embryos \cite{wu_inverted_2011}, mouse brain \cite{dodt_ultramicroscopy:_2007}, and even mouse embryos \cite{ichikawa_live_2013, udan_quantitative_2014, strnad_inverted_2016}.

  As many of these specimens require very different conditions and mounting techniques, these microscopes have been adapted to best accommodate them. An upright objective arrangement (\autoref{fig:spim_zoo}b), for example, allows imaging samples on a coverslip, while its inverted version is well suited for mouse embryos, where a foil is separating the samples from the immersion medium (\autoref{fig:spim_zoo}c). A modified version of the upright arrangement allows for multi-view imaging using both objectives for illumination and detection in a sequential manner (\autoref{fig:spim_zoo}d) \cite{kumar_dual-view_2014}.

  To achieve a more even illumination in larger samples, two objectives can be used from opposing directions to generate two light-sheets (\autoref{fig:spim_zoo}e) \cite{huisken_even_2007}. This arrangement can further be complemented by a second detection objective, to achieve parallelized multi-view imaging (\autoref{fig:spim_zoo}f) \cite{krzic_multiview_2012,tomer_quantitative_2012, schmid_high-speed_2013}. For ultimate speed, 4 identical objectives can be used to achieve almost instantaneous views from 4 different directions by using all objectives for illumination and detection (\autoref{fig:spim_zoo}g) \cite{chhetri_whole-animal_2015}. 

  Furthermore, because of the wide-field detection scheme it is possible to combine SPIM with many superresolution techniques, such as single molecule localization \cite{cella_zanacchi_live-cell_2011}, STED \cite{friedrich_sted-spim:_2011}, RESOLFT \cite{hoyer_breaking_2016}, or structured illumination \cite{keller_fast_2010,chen_lattice_2014,chang_csilsfm_2017}.




  % \subsection{Optics of light-sheet microscopy}

    
    \begin{figure}[htb]
        \centering
        \includegraphics[page=1,width=0.7\textwidth]{spim_cyl}
        \bcaption[Basic optical components of a SPIM]{A dedicated illumination objective is used to generate the light-sheet, which is an astigmatic Gaussian beam, focused along one direction. Astigmatism is introduced by placing a cylindrical lens focusing on the back focal plane of the objective. Detection is performed at a right angle, with a second, detection objective. Scattered laser light is filtered out, and a tube lens forms the image on an area sensor, such as an sCMOS camera.}
        \label{fig:light-sheet}
    \end{figure}

    % \begin{figure}
    %     \centering
    %     \includegraphics[width=1\textwidth]{psfs/psf_spim.pdf}
    %     \bcaption[Axial cross section of the PSF in a light-sheet microscope]{Simulated PSFs for illumination (left, NA = 0.1, $\lambda = \SI{488}{nm}$) and detection (center, NA=1.1, $\lambda = \SI{510}{nm}$). The system PSF is the multiplication of these PSFs (right).($n=1.33$). Because of the illumination scheme, the light-sheet microscope has a better axial resolution than a conventional wide-field setup.}
    %     \label{fig:psf-spim}
    % \end{figure}


  % \subsection{Detection}
    Since illumination and detection for light-sheet microscopy are decoupled, two independent optical paths are implemented.
    
    The detection unit of a SPIM is largely equivalent to a detection unit of a wide-field microscope without the dichroic mirror (\autoref{fig:light-sheet}). The most important components are the objective together with the tube lens, filter wheel, and a sensor, typically a charge coupled device (CCD) or scientific complementary metal–oxide–semiconductor (sCMOS) camera.
    
    The resolution of a light-sheet microscope mainly depends on the detection objective.
    % One of the most important aspects that determines the resolution of the microscope is the detection objective.
    Since imaging biological specimens usually requires a water-based solution, the objectives also need to be directly submerged in the medium to minimize spherical aberrations. As the refraction index of water ($n=1.33$) is greater than the refraction index of air, these objectives tend to have a higher NA, resulting in higher resolution. As the illumination is decoupled in this system, the light-sheet thickness also has an influence on the axial resolution. Finally, the resolution also depends on the pixel size of the sensor which determines the spatial sampling rate of the image.

    Although image quality and resolution greatly depend on the detection optics, the real strength of light-sheet microscopy is the inherent optical sectioning which is due to the specially aligned illumination pattern that confines light to the vicinity of the detection focal plane.

    There are two most commonly used options to generate a light-sheet: either by using a cylindrical lens to illuminate the whole field of view with a static light-sheet, as in the original SPIM concept \cite{huisken_optical_2004}; or by quickly scanning a thin laser beam through the focal plane, thus resulting in a virtual light-sheet. This method is called Digitally Scanned Light-sheet Microscopy (DSLM) \cite{keller_reconstruction_2008}.


  % \subsection{Illumination}

  \subsection{Static light-sheet illumination}
    For a static light-sheet, the normally circular Gaussian laser beam needs to be shaped in an astigmatic manner, \textit{i.e.}, either expanded or squeezed along one direction, to shape it into a sheet instead of a beam. This effect can be achieved by using a cylindrical lens, which, as the name suggests, has a curvature in one direction, but is flat in the other, thus focusing a circular beam to a sheet (\autoref{fig:light-sheet}).
    
    However, to achieve light-sheets that are sufficiently thin for optical sectioning, one would need to use a cylindrical lens with a very short focal length, and these are hardly accessible in well corrected formats. For this reason, it is more common to use a longer focal length cylindrical lens in conjunction with a microscope objective, which is well corrected for chromatic and spherical aberrations \cite{greger_basic_2007}. This way, the light-sheet length, thickness and width can be adjusted for the specific imaging tasks.


    \subsubsection{Light-sheet dimensions}
    \label{sec:dimensions}
    

    The shape of the illumination light determines the optical sectioning capability and the field of view of the microscope, so it is important to be able to quantify these measures. The most commonly used illumination source is a laser beam coupled to a single mode fiber, thus its properties can be described by Gaussian beam optics.

    For paraxial waves, \textit{i.e.}, waves with nearly parallel wavefront normals, the general wave equation can be approximated with the paraxial Helmholtz equation \cite{saleh_fundamentals_2007}
    \begin{equation}
      \nabla_T^2 U + i 2k \frac{\partial U}{\partial z} = 0,
      \label{eq:helmholtz}
    \end{equation}
    where $\nabla_T^2 = \frac{\partial^2}{\partial x^2} + \frac{\partial^2}{\partial y^2}$, $U(\vec{r})$ is the wave function, $k=\frac{2\pi}{\lambda}$ is the wavenumber and $z$ is in the direction of the light propagation.
    
    A simple solution to this differential equation is the Gaussian beam:
    \begin{equation}
      U(r,z) = A_0 \cdot \frac{W_0}{W(z)} \cdot e^{-\frac{r^2}{W^2(z)}}\cdot e^{-i\cdot \phi(r,z)},
    \label{eq:gaussian}
    \end{equation}
    where $A_0$ is the amplitude of the wave, $W_0$ is the radius of the beam waist (the thinnest location on the beam), $r=\sqrt{x^2+y^2}$ is the distance from the center of the beam, $W(z)$ is the radius of the beam  at distance $z$ from the waist, and $\phi(r,z)$ is the combined phase part of the wave-function. Furthermore:

    \begin{equation}
      W(z) = W_0\sqrt{1+\left( \frac{z}{z_\mathrm{R}} \right)^2}
    \end{equation}
    where the parameter $z_\mathrm{R}$ is called the Rayleigh-range, and is defined the following way:

    \begin{equation}
      z_\mathrm{R} = \frac{\pi W_0^2}{\lambda}.
      \label{eq:rayleigh}
    \end{equation}
    

    % The available height of the field of view depends on the uniformity of the illumination. The intensity of the emitted fluorescence is based on the intensity of the excitation light. In case of a Gaussian beam:
    % \begin{equation}
    %   I(r,z) = U(r,z)\cdot U^*(r,z) = |A_0|^2 \cdot \left( \frac{W_0}{W(z)}\right)^2 \cdot e^{-\frac{2r^2}{W^2(z)}},
    % \end{equation}

    Apart from the circular Gaussian beam, the elliptical Gaussian beam is also an eigenfunction of Helmholtz equation (\autoref{eq:helmholtz}) which describes the beam shape after a cylindrical lens:
    \begin{equation}
      U(x,y,z) = A_0 \cdot \sqrt{\frac{W_{0,x}}{W_x(z-z_{0,x})}} \sqrt{\frac{W_{0,y}}{W_y(z-z_{0,y})}} \cdot e^{-\frac{x^2}{W_x^2(z-z_{0,x})}} \cdot e^{-\frac{y^2}{W_y^2(z-z_{0,y})}} \cdot e^{-i\cdot \phi(x,y,z)}.
    \end{equation}
    This beam still has a Gaussian profile along the $x$ and $y$ axes, but the radii ($W_{0,x}$ and $W_{0,y}$), and the beam waist positions ($z_{0,x}$ and $z_{0,y}$) are uncoupled, which results in an elliptical and astigmatic beam. The beam width can now be described by two independent equations for the two orthogonal directions:
    \begin{align}
      W_x(z) = W_{0,x}\sqrt{1+\left( \frac{z}{z_{\mathrm{R},x}} \right)^2}\mathrm{\quad and \quad } W_y(z) = W_{0,y}\sqrt{1+\left( \frac{z}{z_{\mathrm{R},y}} \right)^2}.
    \end{align}
    Since the beam waist is different along the two axes, the Rayleigh range is also different:
    \begin{align}
      z_{\mathrm{R},x} = \frac{\pi W_{x,0}^2}{\lambda}, \quad \mathrm{and} \quad
      z_{\mathrm{R},y} = \frac{\pi W_{y,0}^2}{\lambda}.
      \label{eq:rayleighXY}
    \end{align}
    % Intensity of the beam is the following:
    % \begin{equation}
    %   I(x,y,z) = U(x,y,z)\cdot U^*(x,y,z) = |A_0|^2 \cdot \frac{W_{x,0}}{W_x(z)} \cdot \frac{W_{y,0}}{W_y(z)} \cdot e^{-\frac{2x^2}{W_x^2(z)}} \cdot e^{-\frac{2y^2}{W_y^2(z)}}
    % \end{equation}
    
    \begin{figure}
      \centering
      \begin{subfigure}[b]{0.39\textwidth}
          \centering
          \includegraphics[width=\textwidth]{FOV}
          \caption{}
          \label{fig:fov}
      \end{subfigure}
      \begin{subfigure}[b]{0.29\textwidth}
          \centering
          \includegraphics[width=\textwidth]{width}
          \caption{}
          \label{fig:width}
      \end{subfigure}
      \begin{subfigure}[b]{0.29\textwidth}
          \centering
          \includegraphics[width=\textwidth]{height}
          \caption{}
          \label{fig:height}
      \end{subfigure}
      \bcaption[Light-sheet dimensions]{(a) The light-sheet, with the field of view indicated. Since the light-sheet intensity is uneven, the field of view has to be confined to a smaller region. (b) The width and thickness of the field of view depends on the Rayleigh length of the beam ($z_{R,y}$) and the beam waist ($W_0$). (c) The height of the field of view is determined by the Gaussian profile of the astigmatic beam. }
      \label{fig:ls_dim}
    \end{figure}

    Based on these equations, the light-sheet dimensions and usable field of view can be specified (\autoref{fig:fov}). The light-sheet thickness will depend on the beam waist, $W_{0,y}$ (if we assume the cylindrical lens is focusing along $y$), and the length of the light-sheet can be defined as twice the Rayleigh range, $2 \cdot z_{\mathrm{R},y}$ (\autoref{fig:width}). As these are coupled (see \autoref{eq:rayleighXY}), having a thin light-sheet for better sectioning also means that its length will be relatively short. Fortunately, because of the quadratic relation, to increase the field of view by a factor of two, the light-sheet thickness only needs to increase by a factor of $\sqrt{2}$.

    % Since the illumination is uneven, the usable height of the field of view is smaller than the actual illuminated region (\autoref{fig:fov}). The width of the field of view $w_{fov}$ is determined by the Rayleigh length, since this is in a direct relation with the beam divergence. To stay in the optimal region, the light-sheet should only be used in the range of 1 Rayleigh length on both sides of the beam waist (\autoref{fig:width}). In this range, the ratio between the thickest (at $z=z_0$) and the thinnest (at $z=0$) part of the beam $W(z)$ will be $\sqrt{2}\approx 1.4142$ which is still acceptable.

    Light-sheet height is determined by the intensity profile of the beam along the vertical axis (\autoref{fig:height}). Since this is a Gaussian function (see \autoref{eq:gaussian}), only a small part in the middle can be used for imaging, because towards the sides the intensity dramatically drops. When allowing a maximum 20\% drop-off in intensity at the edges, the light-sheet height becomes $h_{fov}=2\cdot 0.472\cdot W_{x,0} = 0.944 \cdot W_{x,0}$.
    

  \subsection{Digitally scanned light-sheet illumination}

    \begin{figure}[bt!]
      \centering
      \includegraphics[page=2,width=0.7\textwidth]{spim_cyl}
      \bcaption[DSLM illumination]{DSLM illuminates a specimen by a circularly-symmetric beam that is scanned over the field of view. This creates a virtual light-sheet, which illuminates a section of a specimen just like the SPIM. The light-sheet in DSLM is uniform over the whole field of view and its height can be dynamically altered by changing the beam scan range.
    }
        \label{fig:dslm}
    \end{figure}

    Although generating a static light-sheet is relatively straightforward with the simple addition of a cylindrical lens to the light path, it has some drawbacks. As already mentioned in the previous section, the light intensity distribution along the field of view is not constant, as the light-sheet is shaped from a Gaussian beam. Furthermore, along the lateral direction of the light-sheet the illumination NA is extremely low, resulting in effectively collimated light. Because of this, shadowing artifacts can deteriorate the image quality \cite{huisken_even_2007}.

    A more flexible way of creating a light-sheet is by scanning a focused beam in the focal plane to generate a virtual light-sheet (digital scanned light-sheet microscopy, DSLM \cite{keller_reconstruction_2008}). Although this method might require higher peak intensities, it solves both drawbacks of the cylindrical lens illumination. By scanning the beam, the light-sheet height can be freely chosen, and a homogenous illumination will be provided. Focusing the beam in all directions evenly introduces more angles in the lateral direction as well, which shortens the length of the shadows.
    
    The basic optical layout of a DSLM is shown on \autoref{fig:dslm}. A galvanometer controlled mirror that can quickly turn around its axis is used to alter the beam path, which will result in an angular sweep of the laser beam. To change the angular movement to translation, a scan lens is used to generate an intermediate scanning plane. This plane is then imaged to the specimen by the tube lens and the illumination objective, resulting in a scanned focused beam at the detection focal plane. The detection unit is identical to the wide-field detection scheme, similarly to the static light-sheet illumination. By scanning the beam at a high frequency, a virtual light-sheet is generated, and the fluorescence signal is captured by a single exposure on the camera, resulting in an evenly illuminated field of view.

  



  



% ##     ##  #######  ##     ##  ######  ######## 
% ###   ### ##     ## ##     ## ##    ## ##       
% #### #### ##     ## ##     ## ##       ##       
% ## ### ## ##     ## ##     ##  ######  ######   
% ##     ## ##     ## ##     ##       ## ##       
% ##     ## ##     ## ##     ## ##    ## ##       
% ##     ##  #######   #######   ######  ######## 
\section{Light-sheet imaging of mammalian development}

  Live imaging of mammalian embryos is an especially challenging task due to the intrauterine nature of their development. As the embryos are not accessible in their natural environment, it is necessary to replicate the conditions as closely as possible by providing an appropriate medium, temperature, and atmospheric composition. Moreover, these embryos are extremely sensitive to light, which poses a further challenge for microscopy \cite{nowotschin_chapter_2010}.  lllumination with high laser power for an extended time frame can result in bleaching of the fluorophores, which in turn will lower the signal at later times. Furthermore, any absorbed photon has the possibility to modify the chemical bonds inside the specimen, which can lead to phototoxic effects, disrupting the proper development of the embryo.

  Because of its optical sectioning capabilities combined with the high specificity of fluorescent labels, confocal microscopy has had an immense influence on biological research, and has been the go-to technique for decades for many discoveries \cite{shotton_confocal_1989,graf_live_2005,jonkman_any_2015}.
  % Even though it has some drawbacks, important discoveries have been made using this technique, as live imaging is not always necessary.
  % Especially using cell cultures, or fixed specimens, confocal microscopy has been used to investigate X chromosome inactivation \cite{costanzi_histone_1998}, the role of amyloid $\upbeta$-peptide in Alzheimer's disease \cite{bard_peripherally_2000}, the role of Oct4 \cite{nichols_formation_1998} and Sox2 \cite{avilion_multipotent_2003} in the self-renewal of pluripotent stem cells, just to highlight a few.
  %Live imaging capabilities of confocal microscopy are, however, limited.
  Imaging live specimens for an extended period of time with confocal microscopy, although possible \cite{aldaz_live_2010, maitre_asymmetric_2016}, is not ideal. Due to the use of a single objective, for each voxel imaged, a large portion of the specimen has to be illuminated below and above the focal plane as well. This results in a high dose of radiation on the sample that can be as much as 30--100 times larger than the dose used for the actual imaging \cite{reynaud_light_2008}, depending on the number of planes recorded.  
  Moreover, the usage of the pinhole, although rejects out-of-focus light, also decreases the detectable signal intensity, thus it can have a negative impact on image contrast \cite{stelzer_contrast_1998}.

  In contrast to confocal microscopy, light-sheet microscopy uses a much more efficient illumination scheme, as only the vicinity of the focal plane is illuminated. To achieve 3D imaging, the sample is translated relative to the light-sheet, while snapshots are taken from each plane. The total irradiation in this case will be proportional to the thickness of the light-sheet, and will not depend on the number of planes recorded. 

  Another benefit of light-sheet microscopy lies in the parallel readout of the fluorescence due to the wide-field detection scheme. Since the whole focal plane is captured at the same time, this can be considerably faster compared to the point-scanning method of confocal microscopy.
  
  In the next sections we will review the possible strategies for imaging mouse specimens with light-sheet microscopy in different stages of development: pre-implantation and post-implantation embryos, and also adult mice.




  \subsection{Imaging mammalian pre-implantation development}
  \label{ch:mouse-intro}
    Pre-implantation is the first phase of mouse embryonic development that starts right after fertilization. The embryo in this phase is still in the oviduct, travelling towards the uterus, where it will implant into the uterine wall. The developmental stage between fertilization and implantation is called the pre-implantation stage. Here the embryo divides, and already the first cell fate specifications start when forming the trophoectoderm (TE) and the inner cell mass (ICM) at the blastocyst stage. ICM cells will form the embryo proper, while TE cell will contribute to the formation of the extraembryonic tissues.
    
    During this process the embryo is still self-sufficient, which makes it possible to image this stage in an \textit{ex vivo} embryo culture by providing the proper conditions \cite{doherty_culture_2000}. Long term imaging, however, is extremely challenging due to the very high light sensitivity of the specimens. Imaging these embryos in a confocal microscope will lead to incomplete development, even if the imaging frequency is minimized to every \SI{15}{mins} \cite{strnad_inverted_2016}.

    Imaging for just a few hours is already enough to investigate important processes, such as cell fate patterning \cite{dietrich_stochastic_2007}. Other approaches aim to lower the phototoxicity by either using 2-photon illumination which operates at longer wavelengths \cite{denk_two-photon_1990,squirrell_long-term_1999,mcdole_lineage_2011}, or by lowering imaging frequency as a compromise \cite{yamagata_long-term_2009}. These approaches, however, either require highly specialized equipment, such as an ultra-short pulsed laser, or are compromising on the time resolution.

    Light-sheet microscopy, on the other hand, drastically lowers the phototoxic effects by using a much more efficient illumination scheme (see \autoref{sec:light-sheet}), and thus makes a better use of the photon budget. Using this technique, it is possible to image the full pre-implantation development at high spatial and temporal resolution without any negative impact on the developmental process. Such a microscope was developed by Strnad \textit{et al.} at EMBL \cite{strnad_inverted_2016}, who used it to understand when exactly the first cell fate specification is decided in the embryonic cells.

    \begin{figure}[tb]
      \centering
      \includegraphics[width=0.8\textwidth]{mammals/Figure2}
      \bcaption[Inverted light-sheet microscope for multiple early mouse embryo imaging.]{(i) A sample holder (SH), containing a transparent FEP membrane (M) allows multiple embryo samples (S) to be placed in line for multisample imaging. (ii) Inverted objective orientation with side view of the sample holder. One possible configuration is to use a 10× 0.3 NA illumination objective (IL) and another 100× 1.1 NA detection objective placed at a right angle to the illumination. (iii) Close up on side view of sample on FEP membrane with both objectives. Since the FEP membrane is transparent on water, it provides no hindrance to the illumination beam in penetrating the sample or for the emitted fluorescence on reaching the detection objective. (B) Still images of one particular timelapse experiment, and (C) corresponding segmented nuclei. The star depicts the polar body. Adapted from Strnad \etal \cite{strnad_inverted_2016}}
      \label{fig:preMouse}
    \end{figure}

    
    As a mouse embryo culture is not compatible with the standard agarose-based sample mounting techniques, a completely new approach was taken, which resulted in a microscope designed around the sample. The sample holder forming a V-shape was built with a bottom window, and it is lined with a thin FEP (fluorinated ethylene propylene) foil that supports the embryos (\autoref{fig:preMouse}A, i). This arrangement allows the utilization of the standard microdrop embryo culture, while providing proper viewing access for the objectives. As the embryos are relatively small (\SI{100}{\micro m}) and transparent, a single illumination and single detection objective arrangement is enough for high quality imaging. A low resolution (NA=0.3) objective is used to generate the scanned light-sheet, and a high resolution (NA=1.1) objective is detecting the fluorescence at 50\texttimes magnification (\autoref{fig:preMouse}A, ii). As the foil is curved, it allows unrestricted access to the embryo, while separating the imaging medium from the immersion liquid (\autoref{fig:preMouse}A, iii). Furthermore, its refractive index is matching the refractive index of water, so optical aberrations are minimized.

    Using this setup, Strnad \textit{et al.} were able to pinpoint the exact timing of the first cell fate decision that leads either to ICM or TE cells. More than 100 embryos expressing nuclear (H2B-mCherry) and membrane (mG) markers were imaged for the entire 3 days of pre-implantation development (\autoref{fig:preMouse}B). The image quality was sufficient to segment all nuclei in the embryos (\autoref{fig:preMouse}C), and track them from 1 to 64 cell stage, building the complete lineage tree. Based on the lineage trees and the final cell fate assignments, it was determined that at the 16 cell stage the final specification is already decided, while earlier than this it is still random.
    




  \subsection{Imaging mammalian post-implantation development}
  
    \begin{figure}[tb]
      \centering
      \includegraphics[width=0.8\textwidth]{mammals/Figure3}
      \bcaption[Imaging mouse post-implantation development]{(A) (i, ii) Mounting technique for E5.5 to E6.5 embryos. A tip-truncated \SI{1}{mL} syringe holds an acrylic rod, cut and drilled with holes of different size in order to best fit the mouse embryo by its Reichert's membrane, leaving the embryo free inside the medium. (iii) Maximum intensity projection of a \SI{13}{\micro m} thick slice at \SI{78}{\micro m} from distal end of an E6.5 mouse embryo. The different tissues corresponding to the rudimentary body plan are annotated. Scale bar: \SI{20}{\micro m}. (B) For stages ranging between E6.5 and E8.5, mounting using a hollow agarose cylinder has also successfully been proposed. Optimal sizes for the corresponding embryonic stage to be imaged can be produced, so that the embryo can grow with least hindrance. (C–F) Steps for mounting the mouse embryo inside the agarose cylinder. The inner volume of the cylinder can be filled with optimal medium, allowing the much larger chamber volume to have less expensive medium. (G–H) Example images of a \SI{9.8}{h} timelapse with the mounting shown in (B) where the expansion of the yolk sac can be observed in direction of the blue arrows. (I) In order to aid multiview light-sheet setups in overcoming the higher scattering properties of embryos at this stage, and to allow faster and easier data recording, electronic confocal slit detection allows better quality images to be taken at shorter acquisition times. Scale bar: \SI{20}{\micro m}. Adapted from Ichikawa \textit{et al.} \cite{ichikawa_live_2013}, Udan \textit{et al.} \cite{udan_quantitative_2014} and de Medeiros and Norlin \textit{et al.} \cite{de_medeiros_confocal_2015}.}
      \label{fig:postMouse}
    \end{figure}

    After the initial 3 days of pre-implantation, the embryo undergoes the implantation process, during which it is inaccessible to microscopical investigations. Although a new method was recently developed that allows the \textit{in vitro} culturing of the embryos embedded in a 3D gel \cite{panavaite_3d-geec:_2017}, this has not reached wider adoption yet. Hence, developmental processes during implantation have only been investigated in fixed embryos.
    
    Following the implantation process, at the post-implantation phase, \textit{ex vivo} embryo culturing becomes possible again \cite{hsu_vitro_1979, huang_effect_2001}, and these embryos can be kept alive for several days in an artificial environment. During this process especially interesting stages are the late blastocyst ($\sim$E4.5), gastrulation ($\sim$E6.5), and somite formation ($\sim$E8.5). Before live imaging techniques became available, these stages were mostly investigated using \textit{in situ} visualization techniques to shed light on several developmental processes \cite{nowotschin_cellular_2010}. Many pathways playing important roles have been identified this way, however, live imaging is still necessary to validate these results and ensure continuity in the same specimen \cite{garcia_live_2011}.

    Light-sheet microscopy is a good choice for imaging these stages, just like in the case of pre-implantation embryos. These embryos, however, present new challenges for sample handling and culturing. Owing to their extreme sensitivity, dissection can be difficult, especially for earlier stages (E4.5). Furthermore, since the embryo is also growing during development, gel embedding is not an option, as this might constrain proper development. Thus, special handling and mounting techniques had to be developed in order to allow live 3D imaging of these specimens.

    Ichikawa \textit{et al.} \cite{ichikawa_live_2013} designed a custom mounting apparatus manufactured from acrylic in the shape of a rod that fits in a standard \SI{1}{mL} tip-truncated syringe (\autoref{fig:postMouse}A, i). Several holes were drilled in the rod with different sizes, which can accommodate different sized embryos. The embryos are held by an extraembryonic tissue, the Reichert's membrane (\autoref{fig:postMouse}A, ii). Mounting this way does not disturb the embryo itself, and it can freely develop in the culturing medium, while it is also stationary for the purpose of imaging. Using this technique, Ichikawa \textit{et al.} were able to image through several stages of development, including interkinetic nuclear migration at stages E5.5--6.5 (\autoref{fig:postMouse}A, iii).

    A second method of sample mounting for light-sheet imaging was developed by Udan~\etal who were able to record a full \SI{24}{h} time-lapse of living embryos focusing on the gastrulation and yolk sac formation processes (\autoref{fig:postMouse}G--I). Their mounting technique comprised of producing a hollow agarose container shaped like a cylinder that could support the embryo from below without constraining its growth (\autoref{fig:postMouse}B--F).

    Another consideration to keep in mind, is the growing size of the embryo. As it gets bigger, illumination is less efficient, and scattering can dominate at larger depths. As mentioned in earlier (\autoref{sec:multiview}) this can be alleviated by multi-view imaging: illuminating and detecting from multiple directions. Electronic confocal slit detection can further improve the signal-to-noise ratio by rejecting unwanted scattered light, which allows deeper imaging in large specimens, even up to E7.5 (\autoref{fig:postMouse}I) \cite{de_medeiros_confocal_2015}.



  \subsection{Imaging adult mice}
    
    \begin{figure}
      \centering
      \includegraphics[width=0.8\textwidth]{mammals/Figure4}
      \bcaption[Imaging adult mouse brain with light-sheet microscopy.]{(A) Schematics of the ultramicroscope for brain imaging. The specimen is embedded in clearing medium to ensure necessary imaging depth. Illumination is applied from two sides to achieve even illumination for the whole field of view. Light-sheet is generated by a slit aperture followed by a cylindrical lens. The specimen is imaged from the top using wide-field detection method. (B) Photograph of the imaging chamber with a mounted cleared specimen and light-sheet illumination. (C) Surface rendering of a whole mouse brain, reconstructed from 550 optical sections. GFP and autofluorescence signal was recorded. Hippocampal pyramidal and granule cell layers are visible in the digital section. Scale bar: \SI{1}{mm}. Objective: Planapochromat 0.5×. (D) Reconstruction of an excised hippocampus from 410 sections. Note that single cell bodies are visible. Scale bar: \SI{500}{\micro m}. Objective: Fluar 2.5×. (E) 3D reconstruction of a smaller region of an excised hippocampus from 132 sections. Scale bar: \SI{200}{\micro m}. Objective: Fluar 5×. (F) 3D reconstruction of CA1 pyramidal cells imaged with a higher resolution objective (LD-Plan Neofluar 20× NA 0.4) in a whole hippocampus (430 sections). Dendritic spines are also visible, even though usually a higher NA objective (>1.2) is required to visualize these. Scale bar: \SI{5}{\micro m}. Adapted from Dodt \textit{et al.} \cite{dodt_ultramicroscopy:_2007}.}
      \label{fig:adultMouse}
    \end{figure}

    Imaging adult mice is especially interesting for answering neurobiological questions. Since development is over at this stage, the use of an environmental chamber is no longer necessary. The biggest challenge for imaging these samples is their size, as they are centimeters in size instead of less than a millimeter as in the embryonic stage. Furthermore, the tissues of adult mice are much more opaque, which severely limits imaging depth. Light-sheet microscopy can already deal with large specimens, however, to achieve (sub)cellular resolution for an entire brain, for example, multiple recordings have to be stitched together after acquisition \cite{bria_terastitcher_2012}.

    Light scattering and absorption depend on the tissue composition and imaging depth. Especially the brain with a high concentration of lipids in the myelinated fibers pose a real challenge for imaging. Live imaging is usually performed with 2-photon microscopy which can penetrate the tissue up to \SI{800}{\micro m} deep \cite{katona_fast_2012}. Using fixed samples, however, the scattering problem can be eliminated by the use of tissue clearing methods.

    Tissue clearing is a process that removes and/or substitutes scattering and absorbing molecules by a chemical process while keeping the tissue structure intact and preserving fluorescence. The most dominant contributors to these effects are the proteins and lipids. Proteins in the cells locally change the refractive index of the tissue which leads to scattering, while lipids predominantly absorb the light. Clearing methods tackle these problems by chemically removing and substituting lipids by certain types of gel, and immersing the whole sample in a medium with higher refractive index to match the optical properties of proteins. Numerous methods have been developed for tissue clearing, such as ScaleA2 \cite{hama_scale:_2011}, 3DISCO \cite{erturk_three-dimensional_2012,erturk_three-dimensional_2012-1}, SeeDB \cite{ke_seedb:_2013}, CLARITY \cite{chung_clarity_2013}, CUBIC \cite{susaki_whole-brain_2014} and iDISCO \cite{renier_idisco:_2014}.

    The first combination of optical clearing and light-sheet microscopy for whole brain imaging was performed by Dodt \etal using a custom ultramicroscope consisting of two opposing illumination arms and a single detection arm with an objective from above (\autoref{fig:adultMouse}A). The light-sheets were positioned horizontally, and the cleared samples could be placed in a transparent imaging chamber filled with the clearing medium (\autoref{fig:adultMouse}B). Imaging was performed from both top and bottom after rotating the sample \SI{180}{\degree}. By changing the detection lens, it is possible to adapt the system to different samples: low magnification is capable of imaging the whole brain (\autoref{fig:adultMouse}C), while for smaller, dissected parts, such as the hippocampus, higher magnification with higher resolution is more appropriate (\autoref{fig:adultMouse}D). With this configuration individual cell-cell contacts can be recognized (\autoref{fig:adultMouse}E), and even dendritic spines can be visualized (\autoref{fig:adultMouse}F).

    Although light-sheet microscopy is highly suitable for imaging cleared specimens, even entire mice \cite{tainaka_whole-body_2014}, brain imaging in live animals is more challenging due to the two-objective setup of a conventional SPIM microscope. Two light-sheet-based methods, however offer a solution for this, axial plane optical microscopy (APOM) \cite{li_axial_2014} and swept confocally-aligned planar excitation (SCAPE) \cite{bouchard_swept_2015} both use only a single objective to generate a light-sheet and detect the fluorescence as well. This is done by rotating the detection plane at an intermediate image (APOM), or by rotating both the light-sheet and detection plane simultaneously (SCAPE).


