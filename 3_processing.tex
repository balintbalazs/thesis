% !TEX root = dissertation_BB.tex
%% spellcheck-language en-US


  %#######  ########   #######   ######  
  %#     ## ##     ## ##     ## ##    ## 
  %#     ## ##     ## ##     ## ##       
  %#######  ########  ##     ## ##       
  %#        ##   ##   ##     ## ##       
  %#        ##    ##  ##     ## ##    ## 
  %#        ##     ##  #######   ######  

  \graphicspath{{./figures/3_processing/}}

  \section{Image processing for light-sheet microscopy}
  
    necessary preprocessing steps for multi-view light-sheet microscopy
    image fusion
      1. bead based registration
      2. weighted average / cake fusion
      3. multi-view deconvolution
  
  \section{Image compression}
    As we will see later (\autoref{sec:sizes}), light-sheet microscopy is capable of generating an immense amount of data in a short amount of time. This is why not only fast data processing but data compression also plays an important role when evaluating, sharing and storing the images. In this section we will briefly introduce the core concepts of image compression and show some examples as a background for \autoref{ch:GPU}. For a more comprehensive review on data compression fundamentals, the interested reader is referred to the works of Sayood \cite{sayood_introduction_2012}, and Salomon and Motta \cite{salomon_handbook_2010}.
   
    There are two distinct ways of data compression: lossless and lossy. This is an important difference, and greatly depends on the application itself. For some type of data no loss is acceptable, for example computer programs or text. Here, even a small change could drastically change the meaning of the original data. For other applications, mostly audio and video compression, some loss can be tolerated as long as there is no perceived change or distortion. For scientific applications mostly lossless compression is used. Even for image data, where lossy compression can significantly increase the compression ratio (original size / compressed size), lossy compression is usually not recommended, as it can alter the measurement results in unpredictable ways \cite{cromey_digital_2013}.
  
    % There are a multitude of techniques developed in the context of data compression, and most compression methods are built based on this repertoire. Because of this, many algorithms follow a common pattern, and 
  
    \subsection{Basics of information theory}
      As compression is the art of reducing the size of a message without while keeping the same amount of information, it is necessary to briefly discuss the theory behind information, and how to quantify it. These concepts were first introduced by Shannon, in his extremely influential papers \cite{shannon_mathematical_1948,shannon_mathematical_1948-1,shannon_prediction_1951}. As the scope of a rigorous mathematical explanation is outside of the scope of this work, we refer the interested reader to the aforementioned works.
  
      Informally, information can be quantified as the amount of surprise. Any statement, if it has a high probability carries a low amount of information, while if it has a small probability, there is more information. Consider the following example: "It is July". If it is indeed July, this statement doesn't provide too much information. Continuing with "It is snowing outside", carries more information, as it is unexpected under the circumstances (provided the conversation takes place on the Northern Hemisphere).
  
      The self information of event $A$ can be defined the following way:
      \begin{equation}
        I(A) = \log_b \frac{1}{P(A)} = - \log_b P(A),
        \label{eq:selfInfo}
      \end{equation}
      where $P(A)$ is the probability of event $A$ occurring. The base of the logarithm can be chosen freely, but usually the preferred base is $b=2$. With this choice, the unit of information is \SI{1}{bit}, and $I(A)$ represents how many bits are required to store the information contained in $A$.
  
      The average self information, also called the entropy, for a random variable $X$ can be calculated based on the self-information of the outcomes $A_i$:
      \begin{equation}
        H(X) = \sum_i P(A_i)I(A_i) = - \sum_i P(A_i) \log_b P(A_i)
        \label{eq:entropy}
      \end{equation}
      If we consider a data stream $S$, where each symbol records the outcome of independent, identical random experiments $X$, then the average information in each symbol will be $H(X)$. As a consequence, as stated by Shannon's source coding theorem \cite{shannon_mathematical_1948}, the shortest representation of $S$ will require at least $N\cdot H(X)$ bits, where $N$ is the number of symbols in the stream. This defines the theoretical lower bound any nai\:ve compression algorithm (also called entropy coding) can achieve without loosing any information.
  
      The requirement, however, is that the symbols are independent of each other, which is usually not true for most of the data types we (humans) are interested in. In order to achieve ideal compression, the data needs to be modeled, or transformed, to represent it as a sequence of independent variables. This pattern of modeling and coding \cite{rissanen_universal_1981} is the core of all modern compression algorithms. 
      The following sections will briefly introduce these concepts by two examples: Huffman coding, and pixel prediction.
      
      % in the context of image compression
      % redistributing probabilities
      
    \subsection{Entropy coding -- Huffman coding}
      \label{sec:entropyCoding}
      
  
      % The aim of entropy coding (\autoref{sec:entropyCoding}) is, based on the probability distribution of $X$, to compress the data as small as possible, reaching 
  
      % aim: few letters with very high probability, all other letters with very low probability, e.g. exponential distribution
      % worst: uniform distribution = random noise
  
  
      % how to reduce data size? use variable length codes
      % code length depends on symbol frequency - more frequent, shorter codes
      % problem? how do we know the boundaries between codewords?
      % prefix free codes - no codeword is a prefix of any other codeword
      % Huffman coding is like this, and also ideal in the sense that the average codeword length for large data
      % "The long discussion in [Gilbert and Moore 59] \cite{gilbert_variable-length_1959} proves that the Huffman code is a minimum-length code in the sense that no other encoding has a shorter average length"
  
      Huffman coding is a prefix-free, optimal code that is widely used in data compression. It was developed by David A. Huffman as a course assignment on the first ever course on information theory at MIT, and was published shortly afterwards \cite{huffman_method_1952}. It is a variable length binary code which assigns different length codewords to letters of different probabilities. It is able to achieve optimal compression, which means the total length of the coded sequence will be minimal.
      
      Although it produces a variable length code which can introduce some issues with decoding, it is still uniquely decodable. It achieves this property by using prefix-free codewords, meaning that none of the codewords are prefixes of any other codewords. This property can be exploited when decoding the codeword, since during this procedure the number of bits for the next codeword can not be determined in advance. However if no codeword is a prefix of another codeword, by simply reading the successive bits one by one until we reach a valid codeword, it's possible to uniquely decode the message.
  
      Let's take the example in Table \ref{tab:huffman1}. Five letters are coded in binary code by Code \#1 and by Code \#2. Code 1 is not a prefix code, and because of this when reading the encoded sequence we can not be sure when we reach the end of a codeword. Decoding the sequence 0000 for example could be interpreted as 4 letters of $a_1$ or 2 letters of $a_3$.
  
      \begin{table}
        \bcaption[Examples of a random binary code (\#1) and a prefix-free binary code (\#2)]{Code \#2 is uniquely decodable, while for code \#1 it's necessary to introduce boundaries between codewords to be able to distinguish them.}
        \centering
        \begin{tabular}{crr}
          \toprule
          Letter & Code \#1 & Code \#2 \\
          \midrule
          $a_1$ & 0	& 10 \\
          $a_2$ & 11	& 11 \\
          $a_3$ & 00	& 00 \\
          $a_4$ & 10 	& 010 \\
          $a_5$ & 111	& 011 \\
          \bottomrule
        \end{tabular}
        \label{tab:prefix}
      \end{table}
  
      The Huffman coding procedure is based on two observations regarding optimal and prefix-free codes:
      \begin{enumerate}
        \item For a letter with higher frequency the code should produce shorter codewords, and for letters with lower frequency it should produce longer codewords.
        \item In an optimum code, the two least frequent codewords should have the same lengths.
      \end{enumerate}
  
      From these statements the first is trivial to see that is correct. If the more frequent letters would have longer codewords then the less frequent letters, the average codeword length (weighted by the probabilities) would be larger than in the opposite case. Thus, more frequent letters must not have longer codewords than less frequent letter.
  
      The second statement at first glance might not be so intuitive, so let's consider the following situation. The two least frequent codewords do not have the same lengths, that is the least frequent is longer. However, because this is a prefix code, the second longest codeword is not a prefix of the longest codeword. This means, if we truncate the longest codeword to the same length as the second longest, they will still be distinct codes and uniquely decodable. This way we have a new coding scheme which requires less space on average to code the same sequence as the original code, from which we can conclude the original code was not optimal. Therefore, for an optimal code, statement 2 must be true.
  
      \begin{table}
        \bcaption[Huffman code table]{}
        \centering
        \begin{tabular}{ccr}
          \toprule
          Letter & Probability & Codeword \\
          \midrule
          $a_2$ & 0.4 & $c(a_2)$ \\
          $a_1$ & 0.2 & $c(a_2)$ \\
          $a_3$ & 0.2 & $c(a_2)$ \\
          $a_4$ & 0.1 & $c(a_2)$ \\
          $a_5$ & 0.1 & $c(a_2)$ \\
          \bottomrule
        \end{tabular}
        \label{tab:huffman1}
      \end{table}
  
      \begin{figure}
        \centering
        \includegraphics[width=0.6\textwidth]{huffman}
        \bcaption[Building the binary Huffman tree]{The letters are ordered by probability, these will be the final leave of the tree. To join the to branches at every iteration we join the to nodes with the smallest probability, and create a new common node with the sum of the probabilities. This process is continued until all nodes are joined in a root node with probability of 1. Now, if we traverse down the tree to each leaf, the codeword will be defined by their position.}
        \label{fig:huffman}
      \end{figure}
  
      To construct such a code, the following iterative procedure can be used. Let's consider an alphabet with five letters $A = [a_1,a_2,a_3,a_4,a_5]$ with $P(a_1)=P(a_3)=0.2$, $P(a_2)=0.4$ and $P(a_4)=P(a_5)=0.1$ (Table \ref{tab:huffman1}). "The entropy for this source is 2.122 bits/symbol." Let's order the letters by probability, and consider the two least frequent. Since the codewords assigned to these should have the same lengths, "we can assign their codewords as"
      \begin{align*}
        c(a_4) &= \alpha_1 * 0 \\
        c(a_5) &= \alpha_1 *1
      \end{align*}
  
      where $c(a_i)$ is the assigned codeword for letter $a_i$ and $*$ denotes concatenation. Now we define a new alphabet $A'$ with only four letters $a_1, a_2, a_3, a'_4$, where $a'_4$ is a merged letter for $a_4$ and $a_5$ with the probability $P(a'_4) = P(a_4) + P(a_5) = 0.2$. We can continue this process of merging the letters until all of them are merged and we have only one letter left. Since this contains all of the original letters, its probability is 1. We can represent the end result in a binary tree (see Figure \ref{fig:huffman}), where the leaves are the letter of the alphabet, nodes are the merged letters, and the codewords are represented by the path from the root node to each leaf (compare with Table \ref{tab:huffman2}). "The average length of this code is"
      \begin{equation}
        l = 0.4\times 1 + 0.2 \times 2 + 0.2 \times 3 + 0.1 \times 4 + 0.1 \times 4 = 2.2 \text{ bits/symbol}
      \end{equation}
      "A measure of the efficiency of this code is its redundancyâ€”the difference between the entropy and the average length. In this case, the redundancy is 0.078 bits/symbol. The redundancy is zero when the probabilities are negative powers of two."
  
      \begin{table}
        \bcaption[Huffman code table]{}
        \centering
        \begin{tabular}{ccr}
          \toprule
          Letter & Probability & Codeword \\
          \midrule
          $a_2$ & 0.4 & 1 \\
          $a_1$ & 0.2 & 01 \\
          $a_3$ & 0.2 & 000 \\
          $a_4$ & 0.1 & 0010 \\
          $a_5$ & 0.1 & 0011 \\
          \bottomrule
        \end{tabular}
        \label{tab:huffman2}
      \end{table}
  
    \subsection{Transform coding / Modeling / Decorrelation}
      \label{sec:decorrelation}
      Since entropy coding doesn't assume anything about the data structure, it is not capable of recognizing and compressing any regular patterns or correlations between the consecutive data points. To maximize the efficiency of compression, any correlations should be removed from the data before it is fed to the entropy coder. Here we will briefly discuss some decorrelation strategies for image compression that aim to improve the performance of a successive entropy coder.
  
      discrete cosine transform (DCT) \cite{ahmed_discrete_1974}
      discrete wavelet transform (DWT) \cite{mallat_theory_1989, jensen_ripples_2001}
      
      % Decorrelation itself does not compress the data, it just reorganizes the information in a way that make a subsequent entropy coder more efficient.
      
  
      
      To see how such an algorithm could work, let's consider the following sequence:
      \begin{center}
        \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|}
          \hline
          37 & 38 & 39 & 38 & 36 & 37 & 39 & 38 & 40 & 42 & 44 & 46 & 48 \\
          \hline
        \end{tabular}
      \end{center}
      Although there is no obvious pattern in this sequence, there are no sharp jumps between the consecutive values, and this can be exploited by using a prediction scheme. Let's define the prediction ($\Pred(\cdot)$) for each element ($X_k$) to be equal to the preceding element. The prediction error would be $\varepsilon_k = X_K - \Pred(X_k) = X_k - X_{k-1}$:
      \begin{center}
        \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|}
          \hline
          37 & 1 & 1 & -1 & -2 & 1 & 2 & -1 & 2 & 2 & 2 & 2 & 2 \\
          \hline
        \end{tabular}
      \end{center}
      As apparent from this new sequence, the number of distinct values are reduced, which means that fewer bits can represent this sequence than the original. When running these two sequences through an entropy coder, the error sequence will be much more compressed due to this property.
      
      Using a predictive scheme to remove correlations from neighboring elements is a very good strategy for various applications, where this signal is only slowly changing. It has been successfully implemented in audio and image compression algorithms as well, such as lossless-JPEG \cite{pennebaker_jpeg:_1992}, JPEG-LS \cite{weinberger_loco-i_2000}, and also CALIC \cite{wu_context-based_1997}, SFALIC \cite{starosolski_simple_2007}, FLIC \cite{wang_fast_2012}.
  
      \begin{center}
        \begin{tabular}{|c|c|c|c|c}
          \hline
          \rowcolor{gray!25}
          \ \ \ & \ & \ & \ \ \ & \ \\ \hline
          \rowcolor{gray!25}
          \ & C & B & D & \ \\ \hline
          \cellcolor{gray!25} \ & \cellcolor{gray!25}A & \cellcolor{green!25}X & \ & \ \\ \hline
          \ & \ & \ & \ & \ \\
        \end{tabular}
      \end{center}
  
  
  
  
  \section{Noise in light microscopy images}
  \subsection{Photon shot noise}
  \subsection{Camera noise}
    \subsubsection{CCD}
    \subsubsection{EM-CCD}
    \subsubsection{sCMOS}
  \subsection{Variance stabilization}
  